---
title: 'Exercise 5: How many loci are there?'
author: "Matias Gomez"
date: "2/26/2019"
output: html_document
---

### Copying ddRADseq files that have already been demultiplexed to my working dir 
```{bash}
cp -r /home/BIO594/DATA/Week5/*fq.gz ./ 
```

### Checking out files
```{bash}
ls -l
```
Following the Assembly tutorial in http://ddocent.com/assembly/
First, we are going to create a set of unique reads with counts for each individual
```{bash}
ls *.F.fq.gz > namelist
sed -i'' -e 's/.F.fq.gz//g' namelist
AWK1='BEGIN{P=1}{if(P==1||P==2){gsub(/^[@]/,">");print}; if(P==4)P=0; P++}'
AWK2='!/>/'
AWK3='!/NNN/'
PERLT='while (<>) {chomp; $z{$_}++;} while(($k,$v) = each(%z)) {print "$v\t$k\n";}'

cat namelist | parallel --no-notice -j 8 "zcat {}.F.fq.gz | mawk '$AWK1' | mawk '$AWK2' > {}.forward"
cat namelist | parallel --no-notice -j 8 "zcat {}.R.fq.gz | mawk '$AWK1' | mawk '$AWK2' > {}.reverse"
cat namelist | parallel --no-notice -j 8 "paste -d '-' {}.forward {}.reverse | mawk '$AWK3' | sed 's/-/NNNNNNNNNN/' | perl -e '$PERLT' > {}.uniq.seqs"
```

Let’s sum up the number the within individual coverage level of unique reads in our data set
```{bash}
cat *.uniq.seqs > uniq.seqs
for i in {2..20};
do 
echo $i >> pfile
done
cat pfile | parallel --no-notice "echo -n {}xxx && mawk -v x={} '\$1 >= x' uniq.seqs | wc -l" | mawk  '{gsub("xxx","\t",$0); print;}'| sort -g > uniqseq.data
rm pfile
```

Take a look at the contents of uniqseq.data

```{bash}
less uniqseq.data
```
Plot of number of Unique Sequences with More than X Coverage (Counted within individuals)

```{bash}
gnuplot << \EOF 
set terminal dumb size 120, 30
set autoscale
set xrange [2:20] 
unset label
set title "Number of Unique Sequences with More than X Coverage (Counted within individuals)"
set xlabel "Coverage"
set ylabel "Number of Unique Sequences"
plot 'uniqseq.data' with lines notitle
pause -1
EOF
```
Now we need to choose a cutoff value. We want to choose a value that captures as much of the diversity of the data as possible while simultaneously eliminating sequences that are likely errors. Let’s try 4

```{bash}
parallel --no-notice -j 8 mawk -v x=4 \''$1 >= x'\' ::: *.uniq.seqs | cut -f2 | perl -e 'while (<>) {chomp; $z{$_}++;} while(($k,$v) = each(%z)) {print "$v\t$k\n";}' > uniqCperindv
wc -l uniqCperindv
```

We’ve now reduced the data to assemble down to 12699 sequences! But, we can go even further. Let’s now restrict data by the number of different individuals a sequence appears within.

```{bash}
for ((i = 2; i <= 10; i++));
do
echo $i >> ufile
done

cat ufile | parallel --no-notice "echo -n {}xxx && mawk -v x={} '\$1 >= x' uniqCperindv | wc -l" | mawk  '{gsub("xxx","\t",$0); print;}'| sort -g > uniqseq.peri.data
rm ufile
```

Plot of Number of Unique Sequences present in more than X Individuals

```{bash}
gnuplot << \EOF 
set terminal dumb size 120, 30
set autoscale 
unset label
set title "Number of Unique Sequences present in more than X Individuals"
set xlabel "Number of Individuals"
set ylabel "Number of Unique Sequences"
plot 'uniqseq.peri.data' with lines notitle
pause -1
EOF
```


Again, we need to choose a cutoff value. We want to choose a value that captures as much of the diversity of the data as possible while simultaneously eliminating sequences that have little value on the population scale. Let’s try 4.

```{bash}
mawk -v x=4 '$1 >= x' uniqCperindv > uniq.k.4.c.4.seqs
wc -l uniq.k.4.c.4.seqs
```


Now we have reduced the data down to only 7989 sequences!

Let’s quickly convert these sequences back into fasta format We can do this with two quick lines of code:

```{bash}
cut -f2 uniq.k.4.c.4.seqs > totaluniqseq
mawk '{c= c + 1; print ">Contig_" c "\n" $1}' totaluniqseq > uniq.fasta
```

With this, we have created our reduced data set and are ready to start assembling reference contigs.

First, let’s extract the forward reads.

```{bash}
sed -e 's/NNNNNNNNNN/\t/g' uniq.fasta | cut -f1 > uniq.F.fasta
```

For example, first step of rainbow clusters reads together using a spaced hash to estimate similarity in the forward reads only.
dDocent now improves this by using clustering by alignment via the program CD-hit to achieve more accurate clustering. Custom AWK code then converts the output of CD-hit to match the input of the 2nd phase of rainbow.

```{bash}
cd-hit-est -i uniq.F.fasta -o xxx -c 0.8 -T 0 -M 0 -g 1
```

This code clusters all off the forward reads by 80% similarity. This might seem low, but other functions of rainbow will break up clusters given the number and frequency of variants, so it’s best to use a low value at this step.
This code then converts the output of CD-hit to match the output of the first phase of rainbow.

```{bash}
mawk '{if ($1 ~ /Cl/) clus = clus + 1; else  print $3 "\t" clus}' xxx.clstr | sed 's/[>Contig_,...]//g' | sort -g -k1 > sort.contig.cluster.ids
paste sort.contig.cluster.ids totaluniqseq > contig.cluster.totaluniqseq
sort -k2,2 -g contig.cluster.totaluniqseq | sed -e 's/NNNNNNNNNN/\t/g' > rcluster
```


Use the more, head, and/or tail function to examine the output file (rcluster). It’s important to note that the numbers are not totally sequential and that there may not be 1000 clusters. Try the command below to get the exact number.

```{bash}
cut -f2 rcluster | uniq | wc -l 
```

The next step of rainbow is to split clusters formed in the first step into smaller clusters representing significant variants. Think of it in this way. The first clustering steps found RAD loci, and this step is splitting the loci into alleles. This also helps to break up over clustered sequences.

```{bash}
rainbow div -i rcluster -o rbdiv.out 
```


The output of the div process is similar to the previous output with the exception that the second column is now the new divided cluster_ID (this value is numbered sequentially) and there was a column added to the end of the file that holds the original first cluster ID The parameter -f can be set to control what is the minimum frequency of an allele necessary to divide it into its own cluster Since this is from multiple individuals, we want to lower this from the default of 0.2.

```{bash}
rainbow div -i rcluster -o rbdiv.out -f 0.5 -K 10
```

The third part of the rainbow process is to used the paired end reads to merge divided clusters. This helps to double check the clustering and dividing of the previous steps all of which were based on the forward read. The logic is that if divided clusters represent alleles from the same homolgous locus, they should have fairly similar paired end reads as well as forward. Divided clusters that do not share similarity in the paired-end read represent cluster paralogs or repetitive regions. After the divided clusters are merged, all the forward and reverse reads are pooled and assembled for that cluster.

```{bash}
rainbow merge -o rbasm.out -a -i rbdiv.out
```

A parameter of interest to add here is the -r parameter, which is the minimum number of reads to assemble. The default is 5 which works well if assembling reads from a single individual. However, we are assembling a reduced data set, so there may only be one copy of a locus. Therefore, it’s more appropriate to use a cutoff of 2.

```{bash}
rainbow merge -o rbasm.out -a -i rbdiv.out -r 2
```

The rbasm output lists optimal and suboptimal contigs. Previous versions of dDocent used rainbow’s included perl scripts to retrieve optimal contigs. However, as of version 2.0, dDocent uses customized AWK code to extract optimal contigs for RAD sequencing.

```{bash}
cat rbasm.out <(echo "E") |sed 's/[0-9]*:[0-9]*://g' | mawk ' {
if (NR == 1) e=$2;
else if ($1 ~/E/ && lenp > len1) {c=c+1; print ">dDocent_Contig_" e "\n" seq2 "NNNNNNNNNN" seq1; seq1=0; seq2=0;lenp=0;e=$2;fclus=0;len1=0;freqp=0;lenf=0}
else if ($1 ~/E/ && lenp <= len1) {c=c+1; print ">dDocent_Contig_" e "\n" seq1; seq1=0; seq2=0;lenp=0;e=$2;fclus=0;len1=0;freqp=0;lenf=0}
else if ($1 ~/C/) clus=$2;
else if ($1 ~/L/) len=$2;
else if ($1 ~/S/) seq=$2;
else if ($1 ~/N/) freq=$2;
else if ($1 ~/R/ && $0 ~/0/ && $0 !~/1/ && len > lenf) {seq1 = seq; fclus=clus;lenf=len}
else if ($1 ~/R/ && $0 ~/0/ && $0 ~/1/) {seq1 = seq; fclus=clus; len1=len}
else if ($1 ~/R/ && $0 ~!/0/ && freq > freqp && len >= lenp || $1 ~/R/ && $0 ~!/0/ && freq == freqp && len > lenp) {seq2 = seq; lenp = len; freqp=freq}
}' > rainbow.fasta
```

Though rainbow is fairly accurate with assembly of RAD data, even with high levels of INDEL polymorphism. It’s not perfect and the resulting contigs need to be aligned and clustered by sequence similarity. We can use the program cd-hit to do this.

The -M and -T flags instruct the program on memory usage (-M) and number of threads (-T). Setting the value to 0 uses all available. The real parameter of significan is the -c parameter which sets the percentage of sequence similarity to group contigs by. The above code uses 90%. Try using 95%, 85%, 80%, and 99%. Since this is simulated data, we know the real number of contigs, 1000. By choosing an cutoffs of 4 and 4, we are able to get the real number of contigs, no matter what the similarty cutoff.

```{bash}
cd-hit-est -i rainbow.fasta -o referenceRC.fasta -M 0 -T 0 -c 0.9
```

This simple bash script called remake_reference.sh that will automate the process of remaking the reference with a cutoff of 20 copies of a unique sequence to use for assembly and a final clustering value of 90%. It will output the number of reference sequences and create a new, indexed reference with the given parameters. 

```{bash}
curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/remake_reference.sh
```

You can remake a reference by calling the script along with a new cutoff value and similarity.

```{bash}
bash remake_reference.sh 4 4 0.90 PE 2
```

What you choose for a final number of contigs will be something of a judgement call. However, we could try to heuristically search the parameter space to find an optimal value. Download the script to automate this process

Take a look at the script ReferenceOpt.sh.
This script uses different loops to assemble references from an interval of cutoff values and c values from 0.8-0.98.

```{bash}
curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/ReferenceOpt.sh
bash ReferenceOpt.sh 4 8 4 8 PE 16
```

Let’s examine the reference a bit.

```{bash}
bash remake_reference.sh 4 4 0.90 PE 2
head reference.fasta
```

We can use simple shell commands to query this data. Find out how many lines in the file (this is double the number of sequences)

```{bash}
wc -l reference.fasta
```

Find out how many sequences there are directly by counting lines that only start with the header character “>”

```{bash}
mawk '/>/' reference.fasta | wc -l 
```

We can test that all sequences follow the expected format.

```{bash}
mawk '/^NAATT.*N*.*CCGN$/' reference.fasta | wc -l
grep '^NAATT.*N*.*CCGN$' reference.fasta | wc -l
```

So there are 1235 loci